---
title: "Day 10 â€“ Fine tuning du RAG server"
date: 2026-01-09
tags: ["mo5", "rag", "ai", "mcp", "docker", "vps"]
description: "DixiÃ¨me jour de vibe coding : DÃ©ploiement et fine tuning du RAG server"
draft: false
weight: 11
translationKey: "day-10"
---

Dans la continuitÃ© de lâ€™[Ã©pisode prÃ©cÃ©dent](/mo5-blog/days/day-6-rag-server/), je voulais aller un cran plus loin :  
**dÃ©ployer mon RAG server sur Internet**.

Lâ€™objectif Ã©tait double :

- rendre le serveur accessible depuis lâ€™extÃ©rieur
- permettre aux *coding agents* (Copilot, Augment, etc.) dâ€™avoir un **contexte prÃ©cis** pour aider au dÃ©veloppement sur le MO5 (le projet est dÃ©crit ici : https://retrocomputing-ai.cloud/)

---

## DÃ©ployer un RAG, mais pas seulement une API

Pour faire le lien entre les agents de code et mon RAG server, jâ€™ai crÃ©Ã© un **serveur MCP**.  
Il sert dâ€™interface standardisÃ©e entre les outils dâ€™IA (Copilot, Augment, etc.) et mon API RAG.

En pratique, la mise en place du MCP server a Ã©tÃ© presque triviale.  
Je nâ€™ai **rien eu Ã  modifier de fondamental dans le RAG** : toutes les briques existaient dÃ©jÃ .

Le MCP server nâ€™est ni un nouveau moteur, ni une surcouche complexe.  
Câ€™est simplement une **interface / protocole spÃ©cifique** qui joue le rÃ´le dâ€™intermÃ©diaire :

- il reÃ§oit les requÃªtes structurÃ©es du coding agent
- les adapte au format attendu par lâ€™API RAG
- renvoie les rÃ©ponses dans un format directement exploitable par lâ€™agent

Toute lâ€™intelligence reste donc cÃ´tÃ© RAG.  
Le MCP ne fait que **traduire et orchestrer**.

Quitte Ã  exposer quelque chose sur Internet, je ne voulais pas juste une API brute.  
Avec un nom de domaine, autant en profiter pour :

- hÃ©berger un petit site HTML expliquant comment configurer le MCP avec Copilot ou Augment
- exposer proprement lâ€™API
- documenter tout Ã§a avec Swagger

Les sources du MCP server, ainsi quâ€™une page Markdown expliquant son fonctionnement, sont disponibles ici :

ğŸ‘‰ https://github.com/thlg057/mo5-mcp-server

---

## Choix de lâ€™hÃ©bergement

Je voulais pouvoir dÃ©ployer **mon image Docker** simplement.  
En regardant ce qui se faisait, je suis rapidement arrivÃ© sur les **VPS**.

AprÃ¨s comparaison des prix et des configurations, je suis parti sur :

- **Hostinger â€“ offre KVM1**
- 4,99 â‚¬ / mois
- 1 vCPU
- 4 Go de RAM

Ce nâ€™est pas une grosse config, mais Ã§a devrait Ãªtre suffisant pour mon usage.  
Bonus apprÃ©ciable : **le nom de domaine offert**, parfait pour donner une vraie URL au projet.

![Hostinger](/assets/hostinger.png "Hostinger hosting")

---

## Architecture de dÃ©ploiement

Pour la gestion des URLs, jâ€™ai choisi **Caddy**.

Lâ€™idÃ©e est simple :

- `/` â†’ redirection vers mon blog
- `/api` â†’ mon API RAG
- `/swagger` â†’ documentation interactive de lâ€™API

CÃ´tÃ© serveur, jâ€™ai prÃ©parÃ© un rÃ©pertoire de dÃ©ploiement avec :

- le blog (Hugo)
- les sources de lâ€™API
- un `docker-compose.yml`
- la configuration Caddy

Copie du tout sur le VPS Hostinger, installationâ€¦  **aucun souci particulier**, câ€™est plutÃ´t simple et bien foutu.

---

## Premier choc : les performances

Sur mon Raspberry Pi 4, jâ€™avais dÃ©jÃ  remarquÃ© un problÃ¨me : **environ 30 secondes** pour obtenir une rÃ©ponse.

Dâ€™aprÃ¨s Augment, le diagnostic Ã©tait clair : le Raspberry nâ€™Ã©tait tout simplement **pas assez puissant**.

Je mâ€™attendais donc Ã  une amÃ©lioration nette sur le VPS.

Premier test en productionâ€¦ **catastrophe** : toujours ~30 secondes, et des rÃ©sultats pas terribles ğŸ˜¬.

Il fallait clairement creuser...

---

## 1. Performance et architecture (vitesse)

### Le problÃ¨me

Au dÃ©part, jâ€™utilisais un service dâ€™**embedding local** :

- le code .NET appelait des scripts Python
- le modÃ¨le Ã©tait chargÃ© Ã  la volÃ©e

RÃ©sultat :
- chargement du modÃ¨le Ã  chaque requÃªte
- CPU saturÃ©
- application instable

Au dÃ©but, je pensais que le problÃ¨me venait de la base de donnÃ©es (mauvaises requÃªtes, index mal optimisÃ©s, etc.).

AprÃ¨s avoir ajoutÃ© pas mal de logs, le verdict est tombÃ© : **la base nâ€™Ã©tait pas en cause**, mais bien la gÃ©nÃ©ration des *chunks* et des embeddings.

### Analyse

Charger un modÃ¨le de deep learning (mÃªme â€œpetitâ€ comme E5) est une opÃ©ration lourde. Le faire Ã  chaque requÃªte est totalement inefficace.

Il fallait une architecture oÃ¹ le modÃ¨le reste **â€œchaudâ€**, chargÃ© une seule fois en mÃ©moire.

### Ce qui a Ã©tÃ© mis en place

- **Micro-service dÃ©diÃ©**  
  Une API Python indÃ©pendante, basÃ©e sur FastAPI, dans son propre conteneur Docker.

- **Chargement unique du modÃ¨le**  
  Le modÃ¨le `multilingual-e5-small` est chargÃ© **une seule fois** au dÃ©marrage du service.

- **Communication HTTP**  
  Le serveur .NET communique dÃ©sormais via des requÃªtes JSON rapides et simples.

### RÃ©sultat

Le temps de traitement est passÃ© :
- de plusieurs secondes **par chunk**
- Ã  **quelques millisecondes**

LÃ , on commence Ã  respirer.

---

## 2. QualitÃ© de la recherche (SimilarityScore)

### Le problÃ¨me

Avec le modÃ¨le initial :
- scores de similaritÃ© autour de **0.60**
- rÃ©sultats souvent peu pertinents
- sections gÃ©nÃ©riques du type *â€œErrors to avoidâ€* qui remontaient tout le temps

Bref, lâ€™IA avait du mal Ã  comprendre les nuances techniques spÃ©cifiques du MO5.

### Analyse

Deux causes principales :

#### 1. Le modÃ¨le
Le modÃ¨le dâ€™origine nâ€™Ã©tait pas assez performant sur :
- le langage technique
- le multilingue (franÃ§ais / anglais)

#### 2. La perte de contexte
Une fois dÃ©coupÃ© en *chunks*, le moteur :
- voyait une suite dâ€™instructions
- mais oubliait **de quel document et de quelle section** elles provenaient

Par exemple, il ne savait plus si on parlait :
- du mode texte
- ou du mode graphique

### Ce qui a Ã©tÃ© mis en place

- **Changement de modÃ¨le**  
  Passage Ã  `intfloat/multilingual-e5-small` â†’ le score de base est passÃ© de **0.61 Ã  0.86**

- **Enrichissement sÃ©mantique**  
  Modification du code C# pour injecter :
  - le titre du document
  - le titre de la section  
  dans chaque chunk envoyÃ© Ã  lâ€™IA

- **Nettoyage du Markdown**  
  Suppression des caractÃ¨res `#`, `**`, etc. pour ne conserver que le texte â€œpurâ€ lors de lâ€™indexation

### RÃ©sultat

Les documents techniques remontent dÃ©sormais **en prioritÃ© nÂ°1** sur les requÃªtes matÃ©rielles (par exemple sur le NMI).
Lâ€™IA â€œcomprendâ€ enfin le **contexte global** de chaque page.

---

## Fine tuning empirique

Jâ€™ai fait pas mal de tests pour affiner le comportement :

- suppression des `#`, `*`, etc.
- taille des chunks
- taille de lâ€™overlap
- ordre des champs injectÃ©s dans le contexte

Tout a Ã©tÃ© fait de maniÃ¨re **empirique**, Ã  coups de tests et de comparaisons.

---

## Aide prÃ©cieuse des IA

Je ne connais pas grand-chose aux modÃ¨les neuronaux.  
Sur ce point :

- **Gemini** mâ€™a beaucoup aidÃ©
  - pour le dÃ©ploiement du service Python
  - pour le choix du modÃ¨le `multilingual-e5-small`

Franchement, sans cette aide, Ã§a aurait Ã©tÃ© beaucoup plus long (et probablement plus douloureux ğŸ˜…).

---

## Tester le RAG en pratique

Pour tester concrÃ¨tement le RAG, le plus simple est de passer par le site :
ğŸ‘‰ https://retrocomputing-ai.cloud/

Câ€™est une page de blog qui explique pas Ã  pas comment utiliser le serveur via un **coding agent** (Copilot, Augment, etc.) en sâ€™appuyant sur le serveur MCP.

Si vous voulez simplement explorer lâ€™API sans passer par un agent, la documentation Swagger est disponible ici :
ğŸ‘‰ https://retrocomputing-ai.cloud/swagger

Vous y trouverez la liste complÃ¨te des endpoints, les formats de requÃªtes et des exemples dâ€™appels pour tester rapidement le RAG.

---

## Ã‰tat actuel de la documentation

La documentation utilisÃ©e par le RAG est encore **en cours de mise Ã  jour**.

Suite Ã  mes derniÃ¨res explorations du code du MO5 â€” et en particulier tout ce qui concerne les **modes graphiques**, je suis en train de revoir et dâ€™enrichir les fichiers de documentation.

Cela signifie que :
- certaines parties sont dÃ©jÃ  trÃ¨s prÃ©cises (notamment sur le matÃ©riel)
- dâ€™autres vont encore Ã©voluer au fil des ajouts
- les rÃ©sultats du RAG vont continuer Ã  sâ€™amÃ©liorer Ã  mesure que la documentation se densifie

Bref, le serveur est opÃ©rationnel, mais le contenu quâ€™il exploite est encore vivant (et câ€™est aussi ce qui rend lâ€™expÃ©rience intÃ©ressante).

---

## Conclusion

Ce dÃ©ploiement mâ€™a permis de comprendre une chose essentielle :

> Un RAG qui â€œfonctionneâ€ nâ€™est pas forcÃ©ment un RAG **utilisable**.

Entre :
- lâ€™architecture
- la performance
- la qualitÃ© des embeddings
- le contexte injectÃ©

il y a Ã©normÃ©ment de paramÃ¨tres Ã  ajuster.

Mais une fois les bons choix faits, le gain est **immÃ©diat** et vraiment satisfaisant.

La suite au prochain Ã©pisode ğŸ™‚
